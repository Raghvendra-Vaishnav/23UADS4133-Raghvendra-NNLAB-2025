{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. WAP to implement a three-layer neural network using Tensor flow library (only, no keras) to classify MNIST handwritten digits dataset. Demonstrate the implementation of feed-forward and back-propagation approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUenwrOxPnfG",
        "outputId": "d26ac0de-fa7c-4628-96a5-a5cd341f7a75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\RAM KISHOR VAISHNAV\\AppData\\Local\\Temp\\ipykernel_7080\\3505980894.py:5: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
            "\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1us/step\n",
            "WARNING:tensorflow:From C:\\Users\\RAM KISHOR VAISHNAV\\AppData\\Local\\Temp\\ipykernel_7080\\3505980894.py:61: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "Epoch: 1 Loss: 0.14160986 Accuracy: 0.9601167\n",
            "Epoch: 2 Loss: 0.089442976 Accuracy: 0.97465\n",
            "Epoch: 3 Loss: 0.06353627 Accuracy: 0.9817167\n",
            "Epoch: 4 Loss: 0.044669565 Accuracy: 0.98768336\n",
            "Epoch: 5 Loss: 0.031446327 Accuracy: 0.992\n",
            "Epoch: 6 Loss: 0.025473943 Accuracy: 0.99366665\n",
            "Epoch: 7 Loss: 0.021985024 Accuracy: 0.99438334\n",
            "Epoch: 8 Loss: 0.014646577 Accuracy: 0.9967667\n",
            "Epoch: 9 Loss: 0.011719816 Accuracy: 0.9977667\n",
            "Epoch: 10 Loss: 0.01420776 Accuracy: 0.9963833\n",
            "Test Accuracy: 0.9769\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Disable eager execution to use TF1-style code (placeholders, sessions, etc.)\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Load and Prepare MNIST Data\n",
        "# ---------------------------\n",
        "# Load the MNIST dataset (handwritten digits 0-9)\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to the range [0,1] and flatten images from 28x28 to 784 features\n",
        "x_train = x_train.reshape(-1, 784).astype(np.float32) / 255.0\n",
        "x_test  = x_test.reshape(-1, 784).astype(np.float32) / 255.0\n",
        "\n",
        "# Convert labels to one-hot vectors (10 classes: digits 0-9)\n",
        "num_classes = 10\n",
        "y_train = np.eye(num_classes)[y_train]\n",
        "y_test  = np.eye(num_classes)[y_test]\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Create Placeholders for Inputs and Labels\n",
        "# ---------------------------\n",
        "# Placeholder for input images (batch size, 784 features)\n",
        "x_ph = tf.compat.v1.placeholder(tf.float32, [None, 784], name=\"x\")\n",
        "# Placeholder for true labels (batch size, 10 classes)\n",
        "y_ph = tf.compat.v1.placeholder(tf.float32, [None, 10],  name=\"y\")\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Define Network Architecture\n",
        "# ---------------------------\n",
        "# Hidden layer with 256 neurons\n",
        "hidden_units = 256\n",
        "W1 = tf.Variable(tf.random.normal([784, hidden_units], stddev=0.1), name=\"W1\")\n",
        "b1 = tf.Variable(tf.zeros([hidden_units]), name=\"b1\")\n",
        "\n",
        "# Output layer with 10 neurons (one per digit class)\n",
        "W2 = tf.Variable(tf.random.normal([hidden_units, 10], stddev=0.1), name=\"W2\")\n",
        "b2 = tf.Variable(tf.zeros([10]), name=\"b2\")\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Build Feed-Forward Computation\n",
        "# ---------------------------\n",
        "# Hidden layer: Linear transformation + ReLU activation function\n",
        "z1 = tf.matmul(x_ph, W1) + b1\n",
        "a1 = tf.nn.relu(z1)\n",
        "\n",
        "# Output layer: Linear transformation (logits)\n",
        "logits = tf.matmul(a1, W2) + b2\n",
        "\n",
        "# Convert logits to probabilities using softmax (for evaluation purposes)\n",
        "predictions = tf.nn.softmax(logits)\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Define Loss and Back-Propagation (Training)\n",
        "# ---------------------------\n",
        "# Compute softmax cross-entropy loss\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_ph, logits=logits))\n",
        "# Define the Adam optimizer to minimize loss\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
        "\n",
        "# ---------------------------\n",
        "# 6. Define Accuracy Metric\n",
        "# ---------------------------\n",
        "# Check if predicted label matches true label\n",
        "correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(y_ph, 1))\n",
        "# Compute mean accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# ---------------------------\n",
        "# 7. Training Parameters\n",
        "# ---------------------------\n",
        "epochs = 10  # Number of training iterations\n",
        "batch_size = 100  # Number of samples per batch\n",
        "num_batches = x_train.shape[0] // batch_size  # Total batches per epoch\n",
        "\n",
        "# ---------------------------\n",
        "# 8. Train the Neural Network\n",
        "# ---------------------------\n",
        "with tf.compat.v1.Session() as sess:\n",
        "    # Initialize all variables\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # (Optional) Shuffle training data at the start of each epoch\n",
        "        indices = np.arange(x_train.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "        x_train = x_train[indices]\n",
        "        y_train = y_train[indices]\n",
        "\n",
        "        # Iterate through mini-batches\n",
        "        for i in range(num_batches):\n",
        "            batch_x = x_train[i*batch_size:(i+1)*batch_size]\n",
        "            batch_y = y_train[i*batch_size:(i+1)*batch_size]\n",
        "            # Perform a single optimization step\n",
        "            sess.run(optimizer, feed_dict={x_ph: batch_x, y_ph: batch_y})\n",
        "\n",
        "        # Evaluate training performance after each epoch\n",
        "        train_loss, train_acc = sess.run([loss, accuracy], feed_dict={x_ph: x_train, y_ph: y_train})\n",
        "        print(\"Epoch:\", epoch+1, \"Loss:\", train_loss, \"Accuracy:\", train_acc)\n",
        "\n",
        "    # Evaluate final performance on the test set\n",
        "    test_acc = sess.run(accuracy, feed_dict={x_ph: x_test, y_ph: y_test})\n",
        "    print(\"Test Accuracy:\", test_acc) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnm2isPLPxhs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
